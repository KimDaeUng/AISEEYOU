{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImagePoemMatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input: 이미지키워드 토크나이징 데이터 (json) + 시 토크나이징 데이터 (json)\n",
    "- output: 이미지 키워드와 시 키워드의 코사인 유사도가 가장 높은 이미지와 시 매칭 (json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 필요한 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import fasttext\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 필요한 데이터 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 토픽 모음\n",
    "image_topic = ['beach', 'cave', 'island', 'lake', 'mountain', 'amusement park', 'palace', 'park', 'restaurant', 'tower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 키워드 토크나이징 데이터 저장 리스트\n",
    "image_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 키워드 토크나이징 데이터 로드\n",
    "for topic in image_topic:\n",
    "    with open('../data/{topic}_token.json'.format(topic=topic), 'rb') as f:\n",
    "        image_tokens.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시 키워드 토크나이징 데이터 로드\n",
    "with open('../data/poem_token.pkl', 'rb') as f:\n",
    "    poem_token = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 테스트 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 코드\n",
    "with open('../data/beach_token.json', 'rb') as f:\n",
    "    beach_keyword_token = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'beach',\n",
       " 'img_name': '1',\n",
       " 'keyword': [['바닷가/NNG_'],\n",
       "  ['모래/NNG_'],\n",
       "  ['바다/NNG_'],\n",
       "  ['물/NNG_', '이/JKS_', '몸/NNG_'],\n",
       "  ['여름/NNG_'],\n",
       "  ['대양/NNG_'],\n",
       "  ['해/NNG_'],\n",
       "  ['재미/NNG_'],\n",
       "  ['분명히/MAG_'],\n",
       "  ['레저/NNG_', '(/SS_', '시간/NNG_', '꺼/VV_', '짐/NNG_', ')/SS_'],\n",
       "  ['여행/NNG_'],\n",
       "  ['휴가/NNG_'],\n",
       "  ['사랑/NNG_'],\n",
       "  ['아이/NNG_'],\n",
       "  ['밀리/VV_', '어/EC_', '오/VX_', '는/ETM_', '파도/NNG_'],\n",
       "  ['자연/NNG_'],\n",
       "  ['연안/NNG_'],\n",
       "  ['천국/NNG_'],\n",
       "  ['자유/NNG_', '(/SS_', '상태/NNP_', ')/SS_'],\n",
       "  ['남성/NNG_'],\n",
       "  ['ᄉ/NNG_', 'ᅩᆨ초해수ᄋ/SH_', 'ᅭᆨ자/NNP_', 'ᆼ/SH_'],\n",
       "  ['ᄀ/NNG_', 'ᅡ족/SH_', '여해/NNP_', 'ᆼ/SH_'],\n",
       "  ['ᄀ/NNG_', 'ᅧ울/SH_', '바ᄃ/NNP_', 'ᅡ/SH_'],\n",
       "  ['ᄉ/NNG_', 'ᅩᆨ초해수ᄋ/SH_', 'ᅭᆨ자/NNP_', 'ᆼ/SH_'],\n",
       "  ['ᄀ/NNG_', 'ᅡ족/SH_', '여해/NNP_', 'ᆼ/SH_'],\n",
       "  ['ᄀ/NNG_', 'ᅧ울/SH_', '바ᄃ/NNP_', 'ᅡ/SH_'],\n",
       "  ['ᄉ/NNG_', 'ᅩᆨ초해수ᄋ/SH_', 'ᅭᆨ자/NNP_', 'ᆼ/SH_'],\n",
       "  ['ᄀ/NNG_', 'ᅡ족/SH_', '여해/NNP_', 'ᆼ/SH_'],\n",
       "  ['ᄀ/NNG_', 'ᅧ울/SH_', '바ᄃ/NNP_', 'ᅡ/SH_'],\n",
       "  ['ᄇ/NNG_',\n",
       "   'ᅡ/SH_',\n",
       "   'ᄃ/SL_',\n",
       "   'ᅡᄂ/SH_',\n",
       "   'ᅳ/NNP_',\n",
       "   'ᆫ/SH_',\n",
       "   '늘/SH_',\n",
       "   '좋다/SH_',\n",
       "   './SF_']]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 모양 확인\n",
    "beach_keyword_token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터에서 명사인 키워드만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_none(x):\n",
    "    d = []\n",
    "    for j in tqdm(x):\n",
    "        c = []\n",
    "        for i in j:\n",
    "            if i == None:\n",
    "                pass\n",
    "            else:\n",
    "                c.append(i)\n",
    "        d.append(c)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_noun_va(x):\n",
    "    new_a = []\n",
    "    for j in tqdm(x):\n",
    "        aa = []\n",
    "        for i in j:   \n",
    "            if 'NNG' in i:\n",
    "                aa.append(i)\n",
    "        new_a.append(aa)\n",
    "    return new_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_keyword(img_data):\n",
    "    key_tok = []\n",
    "    for i in range(len(img_data)):\n",
    "        key_tok.append(img_data[i]['keyword'])\n",
    "    flatten_key_tok = []\n",
    "    for i in key_tok:\n",
    "        flatten_key_tok.append([y for x in i for y in x])\n",
    "    noun_kwd = select_noun_va(del_none(flatten_key_tok))\n",
    "    for i in tqdm(range(len(img_data))):\n",
    "        img_data[i]['keyword'] = 0\n",
    "        img_data[i]['keyword'] = list(set(noun_kwd[i]))\n",
    "    return img_data #noun_keyword로 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. 시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명사 토큰이 없을 때 해당 시를 버리겠다는 의미로 'ㄱ'을 넣어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도연\n",
    "def get_noun_poem(poem_data):\n",
    "    flatten_poem_tok = []\n",
    "    for i in poem_data:\n",
    "        flatten_poem_tok.append([y for x in i for y in x])\n",
    "    noun_poem = select_noun_va(del_none(flatten_poem_tok))\n",
    "    # 시의 경우, 명사 토큰이 없는 경우, 임의로 'ㄱ'을 넣어준다. 해당 시를 버리겠다는 것\n",
    "    for i in range(len(noun_poem)):\n",
    "        if len(noun_poem[i]) == 0:\n",
    "            noun_poem[i] = ['ㄱ']       \n",
    "    return poem_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. fasttext 모델로 word vector 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model('../model/wiki.ko/wiki.ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 코사인 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.sqrt(np.dot(x, x)) * np.sqrt(np.dot(y, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_index(vector_keyword, vector_poem):\n",
    "    scores = []\n",
    "    for i in range(len(vector_poem)):\n",
    "        score = cosine_similarity(vector_keyword, vector_poem[i])\n",
    "        scores.append(score)\n",
    "    index = scores.index(max(scores))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------- 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어의 300차원으로 임베딩된 데이터\n",
    "total = []\n",
    "for i in tqdm(range(len(noun_poem))):\n",
    "    sen = []\n",
    "    total.append(sen)\n",
    "    for j in range(len(noun_poem[i])):\n",
    "        #if noun_poem[i][j] != word:\n",
    "            sen.append(model.get_word_vector(noun_poem[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시 단위로 임베딩 벡터들을 더해준다. 그것을 시에 대한 벡터값으로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_total = np.array([np.array(i) for i in total ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_total_samplesum = [ np.sum(i, axis=0) for i in new_total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_total_samplesum = np.array(new_total_samplesum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_poem = new_total_samplesum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_poem[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### beach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 키워드의 명사 토큰 데이터\n",
    "with open('final_beach.json', 'rb') as f:\n",
    "     json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 키워드에 대해서 300차원으로 임베딩된 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = []\n",
    "for i in tqdm(range(len(json_data))):\n",
    "    sen = []\n",
    "    tot.append(sen)\n",
    "    for j in range(len(json_data[i]['keyword'])):\n",
    "        #if json_data[i]['keyword'][j] != word:\n",
    "            sen.append(model.get_word_vector(json_data[i]['keyword'][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이것 또한 마찬가지로 이미지(keyword) 를 대표하는 하나의 벡터로 나타내준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tot = np.array([np.array(i) for i in tot ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tot_samplesum = [ np.sum(i, axis=0) for i in new_tot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tot_samplesum = np.array(new_tot_samplesum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_keyword = new_tot_samplesum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_keyword[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cosin sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i : keyword_Vector로 이미지에 맞는, 가장 유사도가 높은 poem_vector를 구하고 그 시의 인덱스를 append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for i in range(len(vector_keyword)):\n",
    "    index = return_index(vector_keyword[i], vector_poem)\n",
    "    index_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(index_list))  # 매칭된 시 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for 태욱 최종 json 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 match되는 시를 match_poem에 저장\n",
    "match_poem = []\n",
    "for i in index_list:\n",
    "    match_poem.append(poem_token[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_beach_token.json', 'rb') as f:\n",
    "     final_beach_token = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_data에 넣어준다.\n",
    "for i in tqdm(range(len(match_poem))):\n",
    "    final_beach_token[i]['text'] = 0\n",
    "    final_beach_token[i]['text'] = match_poem[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_beach_token)):\n",
    "    final_beach_token[i]['image'] = final_beach_token[i]['img_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_beach_token)):\n",
    "    del final_beach_token[i]['img_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_beach_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이건lake에 poem이 이미 포함되어 있어서 지우는 코드\n",
    "#for i in range(len(final_lake_token)):\n",
    "#    del final_lake_token[i]['poem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('real_final_beach_token.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_beach_token, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match poem list 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for i in range(len(vector_keyword)):\n",
    "    index = return_index(vector_keyword[i], vector_poem)\n",
    "    index_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(index_list))  # 매칭된 시 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for 태욱 최종 JSON 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 match되는 시를 match_poem에 저장\n",
    "match_poem = []\n",
    "for i in index_list:\n",
    "    match_poem.append(poem_token[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_amusement_token.json', 'rb') as f:\n",
    "     final_amusement_token = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_data에 넣어준다.\n",
    "for i in tqdm(range(len(match_poem))):\n",
    "    final_amusement_token[i]['text'] = 0\n",
    "    final_amusement_token[i]['text'] = match_poem[i]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_amusement_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('real_final_amusement_token.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_amusement_token, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
